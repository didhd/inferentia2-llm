{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74156eb6-9517-475f-984e-2e76d24fb281",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 실시간 엔드포인트를 DJL Serving을 이용해 호스팅\n",
    "이번 노트북에서는 [Deep Java Library](https://djl.ai/) (DJServing)가 지원하는 대형 모델 추론 컨테이너(LMI)를 모델 서빙 솔루션으로 사용합니다. \n",
    "\n",
    "DJL Serving은 별도의 사전 컴파일 없이도 HuggingFace SafeTensor 형식의 모델 웨이트를 자동으로 컴파일하고, 로드할 수 있도록 해 줍니다.\n",
    "\n",
    "AWS Neuron SDK는 사용자가 Inferentia 장치의 강력한 처리 능력을 쉽게 활용할 수 있게 해주며, DJLServing은 Java 기반 환경에서 대규모 모델을 손쉽게 서빙할 수 있도록 지원합니다. 이 노트북은 Amazon Elastic Compute Cloud(Amazon EC2) inf2.xlarge 인스턴스에 Llama 3 모델을 배포합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dff5e52d-2b6d-4e6e-b645-18c4174a8ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.ap-northeast-1.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원하는 리전으로 변경합니다.\n",
    "aws_region = \"ap-northeast-1\"\n",
    "\n",
    "# 컨테이너 이미지 확인\n",
    "image_uri = f\"763104351884.dkr.ecr.{aws_region}.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2\"\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b860ccb2-7580-46ba-a182-c090eac40cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e074430403247e69510de97708cf3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login, snapshot_download\n",
    "\n",
    "# Hugging Face 로그인\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8862d11c-34ac-4da0-8b4a-04d3bbced56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf19c0fd3bc24b8895fc76c0a50044ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/inferentia2-llm/models/llama3'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 다운로드\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "local_model_path = \"./models/llama3\"\n",
    "snapshot_download(repo_id=model_id, local_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a842f02a-3e2e-4d57-ab8e-472d9d89ea90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 서버에 필요한 serving.properties 파일 생성 (로컬 경로 참조)\n",
    "\n",
    "file_content = f\"\"\"engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=/opt/ml/model/llama3\n",
    "option.n_positions=8192\n",
    "option.rolling_batch=vllm\n",
    "option.max_rolling_batch_size=8\n",
    "option.tensor_parallel_degree=8\n",
    "option.enable_mixed_precision_accumulation=true\n",
    "option.load_split_model=false\n",
    "option.group_query_attention=replicated-heads\"\"\"\n",
    "\n",
    "with open(\"serving.properties\",\"w\") as f:\n",
    "    f.write(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f3adc854-4075-48fd-96b5-718aa20b6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 서버에 필요한 serving.properties 파일 생성 (로컬 경로 참조)\n",
    "\n",
    "# file_content = f\"\"\"engine=Python\n",
    "# option.entryPoint=djl_python.transformers_neuronx\n",
    "# option.model_id=/opt/ml/model/llama3\n",
    "# option.batch_size=1\n",
    "# option.neuron_optimize_level=2\n",
    "# option.load_in_8bit=false\n",
    "# option.n_positions=8192\n",
    "# option.rolling_batch=auto\n",
    "# option.tensor_parallel_degree=12\n",
    "# option.dtype=bf16\n",
    "# option.fuse_qkv=true\n",
    "# option.attention_layout=BSH\n",
    "# option.group_query_attention=replicated-heads\n",
    "# option.load_split_model=false\"\"\"\n",
    "\n",
    "# with open(\"serving.properties\",\"w\") as f:\n",
    "#     f.write(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "502450fe-e7a5-4148-9554-d3a39480050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# `serving.properties`를 models/llama3 디렉토리로 이동\n",
    "mkdir -p logs\n",
    "mkdir -p neuron-cache\n",
    "cp serving.properties models/llama3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ece310d1-3922-433d-a21d-9de242e29c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.ap-northeast-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89a77b-8ca1-485d-8611-5d911c84b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO \u001b[m \u001b[92mEc2Utils\u001b[m DJL will collect telemetry to help us better understand our users? needs, diagnose issues, and deliver additional features. If you would like to learn more or opt-out please go to: https://docs.djl.ai/docs/telemetry.html for more information.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Starting model server ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Starting djl-serving: 0.28.0 ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m \n",
      "Model server home: /opt/djl\n",
      "Current directory: /opt/djl\n",
      "Temp directory: /tmp\n",
      "Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.28.0/conf/log4j2.xml -Xmx1g -Xms1g -Xss2m -XX:+ExitOnOutOfMemoryError\n",
      "Number of CPUs: 96\n",
      "Number of Neuron cores: 12\n",
      "Max heap size: 1024\n",
      "Config file: /opt/djl/conf/config.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Default job_queue_size: 1000\n",
      "Default batch_size: 1\n",
      "Default max_batch_delay: 100\n",
      "Default max_idle_time: 60\n",
      "Model Store: /opt/ml/model\n",
      "Initial Models: ALL\n",
      "Netty threads: 0\n",
      "Maximum Request Size: 67108864\n",
      "Environment variables:\n",
      "    HF_HOME: /tmp/.cache/huggingface\n",
      "    SERVING_FEATURES: vllm,lmi-dist,tnx\n",
      "    DJL_CACHE_DIR: /tmp/.djl.ai\n",
      "    OMP_NUM_THREADS: 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning for plugins...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning in plug-in folder :/opt/djl/plugins\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning in plug-in folder :/usr/local/djl-serving-0.28.0/plugins\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin console changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin static-file-plugin changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin cache-engines changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin secure-mode changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin kserve changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin console changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin static-file-plugin changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin cache-engines changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin secure-mode changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin kserve changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m 5 plug-ins found and loaded.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Found model llama3=file:/opt/ml/model/llama3/\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Initializing model: llama3=file:/opt/ml/model/llama3/\n",
      "\u001b[32mINFO \u001b[m \u001b[92mLmiUtils\u001b[m Detected mpi_mode: null, rolling_batch: vllm, tensor_parallel_degree 8, for modelType: llama\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m M-0001: Apply per model settings:\n",
      "    job_queue_size: 1000\n",
      "    max_dynamic_batch_size: 1\n",
      "    max_batch_delay: 100\n",
      "    max_idle_time: 60\n",
      "    load_on_devices: *\n",
      "    engine: Python\n",
      "    mpi_mode: null\n",
      "    option.entryPoint: djl_python.transformers_neuronx\n",
      "    option.load_split_model: false\n",
      "    option.n_positions: 8192\n",
      "    option.tensor_parallel_degree: 8\n",
      "    option.max_rolling_batch_size: 8\n",
      "    option.enable_mixed_precision_accumulation: true\n",
      "    option.group_query_attention: replicated-heads\n",
      "    option.model_id: /opt/ml/model/llama3\n",
      "    option.rolling_batch: vllm\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPlatform\u001b[m Found matching platform from: jar:file:/usr/local/djl-serving-0.28.0/lib/python-0.28.0.jar!/native/lib/python.properties\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python_engine.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/arg_parser.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/aws/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/aws/cloud_watch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/chat_completions/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/chat_completions/chat_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/chat_completions/chat_utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/encode_decode.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/huggingface.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/inputs.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/neuron_utils/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/neuron_utils/model_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/neuron_utils/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/np_util.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/output_formatter.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/outputs.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/pair_list.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/README.md to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/hf_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/lmi_dist_rb_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/scheduler_rb_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/sd_inf2_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/tnx_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/trt_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/vllm_rb_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/request.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/request_io.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/lmi_dist_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/neuron_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/rolling_batch_vllm_utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/scheduler_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/trtllm_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/vllm_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/sagemaker.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/lm_block.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/search_config.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batch_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batcher.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batcher_impl.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/step_generation.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/service_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/session_manager.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/sm_log_filter.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/stable_diffusion_inf2.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/streaming_utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tensorrt_llm.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tensorrt_llm_python.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/test_model.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/optimum_modeling.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/slot.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/speculation.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/token_selector.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/ts_service_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelManager\u001b[m Loading model on Python:[nc0]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model llama3 (M-0001, PENDING) on nc(0) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m M-0001: Available CPU memory: 373942 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model llama3 M-0001 on nc(0)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19000 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=0-7\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set OMP_NUM_THREADS=16\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: 108 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--model-dir', '/opt/ml/model/llama3', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '0']\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stderr: /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stderr:   warnings.warn(\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: PJRT_DEVICE not set, defaulting to NEURON\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: WARNING 07-09 11:37:35 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For distributed inference, please install Ray with `pip install ray`.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: Model loading properties: model_id_or_path='/opt/ml/model/llama3' rolling_batch=<RollingBatchEnum.vllm: 'vllm'> tensor_parallel_degree=8 trust_remote_code=False enable_streaming=<StreamingEnum.false: 'false'> batch_size=8 max_rolling_batch_size=8 dtype=<Dtype.f16: 'fp16'> revision=None output_formatter=None waiting_steps=None mpi_mode=False tgi_compat=False draft_model_id=None spec_length=0 neuron_optimize_level=None enable_mixed_precision_accumulation=None enable_saturate_infinity=None n_positions=8192 unroll=None load_in_8bit=None low_cpu_mem_usage=False load_split_model=False context_length_estimate=None amp='f16' quantize=None compiled_graph_path=None draft_model_compiled_path=None speculative_draft_model=None speculative_length=5 draft_model_tp_size=None task=None save_mp_checkpoint_path=None group_query_attention='replicated-heads' model_loader=None rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'> fuse_qkv=None attention_layout=None collectives_layout=None cache_layout=None partition_schema=None all_reduce_dtype=None cast_logits_dtype=None on_device_embedding_config={}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: Start loading the model /opt/ml/model/llama3 using NeuronAutoModel...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: LLM sharding and compiling started...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: SysHealth: LLM sharding and compilation latency: 42.471760749816895 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 11:38:18 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='/opt/ml/model/llama3', speculative_config=None, tokenizer='/opt/ml/model/llama3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/opt/ml/model/llama3)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: WARNING 07-09 11:38:18 utils.py:465] Pin memory is not supported on Neuron.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [llama3] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerThread\u001b[m Starting worker thread WT-0001 for model llama3 (M-0001, READY) on device nc(0)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Initialize BOTH server with: EpollServerSocketChannel.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m BOTH API bind to: http://0.0.0.0:8080\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:24:13 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:24:21 metrics.py:334] Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 3.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:24:40 metrics.py:334] Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:25:05 metrics.py:334] Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:25:11 metrics.py:334] Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:25:16 metrics.py:334] Avg prompt throughput: 1.4 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:29:37 metrics.py:334] Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:29:42 metrics.py:334] Avg prompt throughput: 34.2 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:29:48 metrics.py:334] Avg prompt throughput: 43.1 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:29:53 metrics.py:334] Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:29:59 metrics.py:334] Avg prompt throughput: 29.0 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:04 metrics.py:334] Avg prompt throughput: 26.2 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:10 metrics.py:334] Avg prompt throughput: 44.6 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:15 metrics.py:334] Avg prompt throughput: 33.1 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:21 metrics.py:334] Avg prompt throughput: 26.3 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:26 metrics.py:334] Avg prompt throughput: 25.3 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:32 metrics.py:334] Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:37 metrics.py:334] Avg prompt throughput: 34.7 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:43 metrics.py:334] Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:49 metrics.py:334] Avg prompt throughput: 25.9 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:30:54 metrics.py:334] Avg prompt throughput: 15.3 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:31:00 metrics.py:334] Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 10.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:31:05 metrics.py:334] Avg prompt throughput: 30.6 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:31:11 metrics.py:334] Avg prompt throughput: 32.7 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:31:16 metrics.py:334] Avg prompt throughput: 26.8 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:31:22 metrics.py:334] Avg prompt throughput: 25.4 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:31:27 metrics.py:334] Avg prompt throughput: 30.6 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:33:20 metrics.py:334] Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:33:27 metrics.py:334] Avg prompt throughput: 76.3 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:33:37 metrics.py:334] Avg prompt throughput: 56.4 tokens/s, Avg generation throughput: 21.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:33:47 metrics.py:334] Avg prompt throughput: 58.5 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:33:53 metrics.py:334] Avg prompt throughput: 41.5 tokens/s, Avg generation throughput: 34.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:34:02 metrics.py:334] Avg prompt throughput: 41.2 tokens/s, Avg generation throughput: 12.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:34:12 metrics.py:334] Avg prompt throughput: 47.1 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:34:22 metrics.py:334] Avg prompt throughput: 62.4 tokens/s, Avg generation throughput: 21.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:34:28 metrics.py:334] Avg prompt throughput: 57.1 tokens/s, Avg generation throughput: 34.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:34:38 metrics.py:334] Avg prompt throughput: 47.9 tokens/s, Avg generation throughput: 12.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:34:47 metrics.py:334] Avg prompt throughput: 61.0 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:34:57 metrics.py:334] Avg prompt throughput: 40.2 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:35:03 metrics.py:334] Avg prompt throughput: 53.4 tokens/s, Avg generation throughput: 34.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:35:13 metrics.py:334] Avg prompt throughput: 52.3 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:35:23 metrics.py:334] Avg prompt throughput: 52.5 tokens/s, Avg generation throughput: 21.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:35:32 metrics.py:334] Avg prompt throughput: 37.6 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:35:38 metrics.py:334] Avg prompt throughput: 40.5 tokens/s, Avg generation throughput: 34.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:36:36 metrics.py:334] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:36:44 metrics.py:334] Avg prompt throughput: 35.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:36:53 metrics.py:334] Avg prompt throughput: 62.5 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:37:03 metrics.py:334] Avg prompt throughput: 58.3 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:37:09 metrics.py:334] Avg prompt throughput: 58.5 tokens/s, Avg generation throughput: 34.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:38:16 metrics.py:334] Avg prompt throughput: 0.1 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:40:43 metrics.py:334] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:40:50 metrics.py:334] Avg prompt throughput: 2388.1 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 75.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:40:55 metrics.py:334] Avg prompt throughput: 1394.4 tokens/s, Avg generation throughput: 40.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:41:03 metrics.py:334] Avg prompt throughput: 1706.6 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:41:13 metrics.py:334] Avg prompt throughput: 1218.2 tokens/s, Avg generation throughput: 24.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:41:18 metrics.py:334] Avg prompt throughput: 1968.0 tokens/s, Avg generation throughput: 41.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:41:27 metrics.py:334] Avg prompt throughput: 1624.7 tokens/s, Avg generation throughput: 3.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:41:36 metrics.py:334] Avg prompt throughput: 1060.0 tokens/s, Avg generation throughput: 28.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:41:42 metrics.py:334] Avg prompt throughput: 1816.1 tokens/s, Avg generation throughput: 34.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:41:50 metrics.py:334] Avg prompt throughput: 1588.3 tokens/s, Avg generation throughput: 4.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 75.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:41:56 metrics.py:334] Avg prompt throughput: 1039.9 tokens/s, Avg generation throughput: 40.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:01 metrics.py:334] Avg prompt throughput: 1321.9 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:06 metrics.py:334] Avg prompt throughput: 1207.2 tokens/s, Avg generation throughput: 42.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:13 metrics.py:334] Avg prompt throughput: 1275.0 tokens/s, Avg generation throughput: 4.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:18 metrics.py:334] Avg prompt throughput: 1996.1 tokens/s, Avg generation throughput: 42.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:27 metrics.py:334] Avg prompt throughput: 2051.0 tokens/s, Avg generation throughput: 3.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 75.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:33 metrics.py:334] Avg prompt throughput: 1513.5 tokens/s, Avg generation throughput: 40.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:40 metrics.py:334] Avg prompt throughput: 1924.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:46 metrics.py:334] Avg prompt throughput: 1463.8 tokens/s, Avg generation throughput: 39.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:52 metrics.py:334] Avg prompt throughput: 1582.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:42:59 metrics.py:334] Avg prompt throughput: 1348.1 tokens/s, Avg generation throughput: 33.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:43:04 metrics.py:334] Avg prompt throughput: 2524.3 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:43:13 metrics.py:334] Avg prompt throughput: 1428.0 tokens/s, Avg generation throughput: 28.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:48:04 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:56:35 metrics.py:334] Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:56:40 metrics.py:334] Avg prompt throughput: 3228.4 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:56:47 metrics.py:334] Avg prompt throughput: 1975.6 tokens/s, Avg generation throughput: 72.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:56:52 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.8 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 75.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:56:57 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 145.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:02 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:12 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:17 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:22 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:27 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:32 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 56.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:37 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 56.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:42 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 56.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:47 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:52 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 56.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:57:57 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 34.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:02 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:12 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:17 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:22 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:27 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:32 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:37 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:42 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:47 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:52 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:58:57 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:59:02 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 12:59:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:06 metrics.py:334] Avg prompt throughput: 34.5 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:11 metrics.py:334] Avg prompt throughput: 3231.6 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:17 metrics.py:334] Avg prompt throughput: 1981.1 tokens/s, Avg generation throughput: 71.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:22 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 181.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:27 metrics.py:334] Avg prompt throughput: 813.6 tokens/s, Avg generation throughput: 135.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:32 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 151.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:37 metrics.py:334] Avg prompt throughput: 818.7 tokens/s, Avg generation throughput: 92.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:42 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.9 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:47 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[1;31mERROR\u001b[m \u001b[92mHttpRequestHandler\u001b[m \n",
      "io.netty.channel.unix.Errors$NativeIoException: recvAddress(..) failed: Connection reset by peer\n",
      "\u001b[1;31mERROR\u001b[m \u001b[92mHttpRequestHandler\u001b[m \n",
      "io.netty.channel.unix.Errors$NativeIoException: recvAddress(..) failed: Connection reset by peer\n",
      "\u001b[1;31mERROR\u001b[m \u001b[92mHttpRequestHandler\u001b[m \n",
      "io.netty.channel.unix.Errors$NativeIoException: recvAddress(..) failed: Connection reset by peer\n",
      "\u001b[1;31mERROR\u001b[m \u001b[92mHttpRequestHandler\u001b[m \n",
      "io.netty.channel.unix.Errors$NativeIoException: recvAddress(..) failed: Connection reset by peer\n",
      "\u001b[1;31mERROR\u001b[m \u001b[92mHttpRequestHandler\u001b[m \n",
      "io.netty.channel.unix.Errors$NativeIoException: recvAddress(..) failed: Connection reset by peer\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:52 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:01:57 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:02 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:12 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:17 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:22 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:27 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:32 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:37 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:42 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:47 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:52 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:02:57 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:02 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:12 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:17 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:22 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:28 metrics.py:334] Avg prompt throughput: 3008.4 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:33 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 75.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:38 metrics.py:334] Avg prompt throughput: 816.7 tokens/s, Avg generation throughput: 127.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:43 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:48 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:53 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:03:58 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:03 metrics.py:334] Avg prompt throughput: 816.2 tokens/s, Avg generation throughput: 127.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:08 metrics.py:334] Avg prompt throughput: 815.8 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:13 metrics.py:334] Avg prompt throughput: 813.5 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:18 metrics.py:334] Avg prompt throughput: 816.6 tokens/s, Avg generation throughput: 127.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:23 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:29 metrics.py:334] Avg prompt throughput: 1426.5 tokens/s, Avg generation throughput: 96.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:34 metrics.py:334] Avg prompt throughput: 817.4 tokens/s, Avg generation throughput: 132.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:39 metrics.py:334] Avg prompt throughput: 817.6 tokens/s, Avg generation throughput: 132.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:44 metrics.py:334] Avg prompt throughput: 816.1 tokens/s, Avg generation throughput: 132.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:49 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:54 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:04:59 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:04 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:09 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:14 metrics.py:334] Avg prompt throughput: 816.5 tokens/s, Avg generation throughput: 130.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:19 metrics.py:334] Avg prompt throughput: 817.4 tokens/s, Avg generation throughput: 130.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:24 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 173.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:29 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:34 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 165.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:39 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:45 metrics.py:334] Avg prompt throughput: 697.5 tokens/s, Avg generation throughput: 134.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:50 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:05:55 metrics.py:334] Avg prompt throughput: 787.1 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:00 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 170.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:05 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:10 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 167.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:15 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:20 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:26 metrics.py:334] Avg prompt throughput: 1542.0 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:31 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:37 metrics.py:334] Avg prompt throughput: 659.2 tokens/s, Avg generation throughput: 134.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:42 metrics.py:334] Avg prompt throughput: 814.1 tokens/s, Avg generation throughput: 126.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:47 metrics.py:334] Avg prompt throughput: 815.9 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:52 metrics.py:334] Avg prompt throughput: 819.0 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:06:57 metrics.py:334] Avg prompt throughput: 1636.5 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:02 metrics.py:334] Avg prompt throughput: 818.5 tokens/s, Avg generation throughput: 128.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:12 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:17 metrics.py:334] Avg prompt throughput: 818.5 tokens/s, Avg generation throughput: 126.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:22 metrics.py:334] Avg prompt throughput: 800.3 tokens/s, Avg generation throughput: 128.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:27 metrics.py:334] Avg prompt throughput: 1578.0 tokens/s, Avg generation throughput: 85.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:32 metrics.py:334] Avg prompt throughput: 816.6 tokens/s, Avg generation throughput: 127.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:37 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 170.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:42 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:47 metrics.py:334] Avg prompt throughput: 1636.4 tokens/s, Avg generation throughput: 84.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:53 metrics.py:334] Avg prompt throughput: 1399.6 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[33mWARN \u001b[m \u001b[92mNettyUtils\u001b[m Channel is closed by peer.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:07:58 metrics.py:334] Avg prompt throughput: 817.1 tokens/s, Avg generation throughput: 122.7 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 75.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:03 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 136.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:08 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:13 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:18 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:23 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:28 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.1 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:33 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:38 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:43 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:48 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:53 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:08:58 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 121.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:03 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:08 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:13 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:18 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:24 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:29 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:34 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:39 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 79.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:44 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:49 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:54 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:09:59 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:10:04 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:10:09 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:10:14 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:10:19 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:10:24 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:11:42 metrics.py:334] Avg prompt throughput: 52.4 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:11:47 metrics.py:334] Avg prompt throughput: 3195.7 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:11:52 metrics.py:334] Avg prompt throughput: 1632.7 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:11:57 metrics.py:334] Avg prompt throughput: 816.3 tokens/s, Avg generation throughput: 135.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:02 metrics.py:334] Avg prompt throughput: 819.1 tokens/s, Avg generation throughput: 134.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:08 metrics.py:334] Avg prompt throughput: 707.5 tokens/s, Avg generation throughput: 140.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:13 metrics.py:334] Avg prompt throughput: 816.9 tokens/s, Avg generation throughput: 134.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:18 metrics.py:334] Avg prompt throughput: 815.8 tokens/s, Avg generation throughput: 134.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:24 metrics.py:334] Avg prompt throughput: 681.6 tokens/s, Avg generation throughput: 139.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:29 metrics.py:334] Avg prompt throughput: 813.0 tokens/s, Avg generation throughput: 133.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:34 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 178.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:39 metrics.py:334] Avg prompt throughput: 817.5 tokens/s, Avg generation throughput: 132.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:44 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 176.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:49 metrics.py:334] Avg prompt throughput: 816.5 tokens/s, Avg generation throughput: 131.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:12:55 metrics.py:334] Avg prompt throughput: 686.3 tokens/s, Avg generation throughput: 133.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:00 metrics.py:334] Avg prompt throughput: 1575.6 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:05 metrics.py:334] Avg prompt throughput: 813.3 tokens/s, Avg generation throughput: 132.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:10 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 176.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:15 metrics.py:334] Avg prompt throughput: 815.7 tokens/s, Avg generation throughput: 131.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:21 metrics.py:334] Avg prompt throughput: 679.1 tokens/s, Avg generation throughput: 138.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:26 metrics.py:334] Avg prompt throughput: 818.8 tokens/s, Avg generation throughput: 130.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:31 metrics.py:334] Avg prompt throughput: 814.3 tokens/s, Avg generation throughput: 129.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:36 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 173.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:42 metrics.py:334] Avg prompt throughput: 1483.7 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:47 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 173.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:52 metrics.py:334] Avg prompt throughput: 814.4 tokens/s, Avg generation throughput: 129.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:13:57 metrics.py:334] Avg prompt throughput: 812.8 tokens/s, Avg generation throughput: 129.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:02 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:07 metrics.py:334] Avg prompt throughput: 815.7 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:12 metrics.py:334] Avg prompt throughput: 815.0 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:17 metrics.py:334] Avg prompt throughput: 1635.8 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:22 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:27 metrics.py:334] Avg prompt throughput: 1634.2 tokens/s, Avg generation throughput: 85.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:32 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:37 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 170.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:42 metrics.py:334] Avg prompt throughput: 817.7 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:47 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 170.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:52 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:14:57 metrics.py:334] Avg prompt throughput: 818.2 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:02 metrics.py:334] Avg prompt throughput: 816.2 tokens/s, Avg generation throughput: 127.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:07 metrics.py:334] Avg prompt throughput: 1636.9 tokens/s, Avg generation throughput: 84.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:12 metrics.py:334] Avg prompt throughput: 814.7 tokens/s, Avg generation throughput: 126.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:17 metrics.py:334] Avg prompt throughput: 814.6 tokens/s, Avg generation throughput: 126.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:22 metrics.py:334] Avg prompt throughput: 1638.3 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:27 metrics.py:334] Avg prompt throughput: 814.6 tokens/s, Avg generation throughput: 129.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:32 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 173.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:37 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:42 metrics.py:334] Avg prompt throughput: 817.7 tokens/s, Avg generation throughput: 128.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:48 metrics.py:334] Avg prompt throughput: 1437.7 tokens/s, Avg generation throughput: 96.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:53 metrics.py:334] Avg prompt throughput: 1630.2 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:15:58 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 166.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:03 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 170.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:08 metrics.py:334] Avg prompt throughput: 813.0 tokens/s, Avg generation throughput: 128.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:13 metrics.py:334] Avg prompt throughput: 813.7 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:18 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 170.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:23 metrics.py:334] Avg prompt throughput: 815.2 tokens/s, Avg generation throughput: 127.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:29 metrics.py:334] Avg prompt throughput: 1376.5 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:34 metrics.py:334] Avg prompt throughput: 1628.5 tokens/s, Avg generation throughput: 85.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:39 metrics.py:334] Avg prompt throughput: 808.4 tokens/s, Avg generation throughput: 134.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:44 metrics.py:334] Avg prompt throughput: 818.4 tokens/s, Avg generation throughput: 131.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:49 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 178.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:54 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 177.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:16:59 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 176.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:04 metrics.py:334] Avg prompt throughput: 817.5 tokens/s, Avg generation throughput: 131.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:09 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 176.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:14 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:20 metrics.py:334] Avg prompt throughput: 817.0 tokens/s, Avg generation throughput: 130.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:25 metrics.py:334] Avg prompt throughput: 1609.3 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:30 metrics.py:334] Avg prompt throughput: 814.1 tokens/s, Avg generation throughput: 131.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:35 metrics.py:334] Avg prompt throughput: 813.6 tokens/s, Avg generation throughput: 130.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:40 metrics.py:334] Avg prompt throughput: 817.6 tokens/s, Avg generation throughput: 130.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:45 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:51 metrics.py:334] Avg prompt throughput: 705.2 tokens/s, Avg generation throughput: 136.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:17:56 metrics.py:334] Avg prompt throughput: 818.2 tokens/s, Avg generation throughput: 130.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:01 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:06 metrics.py:334] Avg prompt throughput: 818.9 tokens/s, Avg generation throughput: 131.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:11 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 177.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:16 metrics.py:334] Avg prompt throughput: 1512.5 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:21 metrics.py:334] Avg prompt throughput: 814.4 tokens/s, Avg generation throughput: 132.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:26 metrics.py:334] Avg prompt throughput: 760.5 tokens/s, Avg generation throughput: 135.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:33 metrics.py:334] Avg prompt throughput: 665.8 tokens/s, Avg generation throughput: 140.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:38 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 170.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:43 metrics.py:334] Avg prompt throughput: 699.5 tokens/s, Avg generation throughput: 137.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:48 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 175.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:53 metrics.py:334] Avg prompt throughput: 1637.1 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:18:58 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:03 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 173.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:08 metrics.py:334] Avg prompt throughput: 813.9 tokens/s, Avg generation throughput: 129.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:15 metrics.py:334] Avg prompt throughput: 656.3 tokens/s, Avg generation throughput: 138.1 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:20 metrics.py:334] Avg prompt throughput: 816.8 tokens/s, Avg generation throughput: 130.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:25 metrics.py:334] Avg prompt throughput: 816.8 tokens/s, Avg generation throughput: 130.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:30 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:35 metrics.py:334] Avg prompt throughput: 1634.0 tokens/s, Avg generation throughput: 85.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:40 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:45 metrics.py:334] Avg prompt throughput: 1469.4 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:50 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:19:55 metrics.py:334] Avg prompt throughput: 817.6 tokens/s, Avg generation throughput: 128.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:00 metrics.py:334] Avg prompt throughput: 816.1 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:05 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:10 metrics.py:334] Avg prompt throughput: 818.3 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:16 metrics.py:334] Avg prompt throughput: 1630.0 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:21 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 171.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:26 metrics.py:334] Avg prompt throughput: 722.8 tokens/s, Avg generation throughput: 132.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:31 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 169.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:36 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 170.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:41 metrics.py:334] Avg prompt throughput: 817.2 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:46 metrics.py:334] Avg prompt throughput: 815.6 tokens/s, Avg generation throughput: 127.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:51 metrics.py:334] Avg prompt throughput: 793.3 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:20:57 metrics.py:334] Avg prompt throughput: 766.9 tokens/s, Avg generation throughput: 132.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:02 metrics.py:334] Avg prompt throughput: 814.6 tokens/s, Avg generation throughput: 129.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:07 metrics.py:334] Avg prompt throughput: 813.0 tokens/s, Avg generation throughput: 129.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:12 metrics.py:334] Avg prompt throughput: 814.0 tokens/s, Avg generation throughput: 129.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:17 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 172.6 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:22 metrics.py:334] Avg prompt throughput: 1598.8 tokens/s, Avg generation throughput: 82.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:27 metrics.py:334] Avg prompt throughput: 819.0 tokens/s, Avg generation throughput: 129.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 87.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:32 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.0 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 75.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:37 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 139.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:42 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:47 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:52 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.7 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:21:57 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:02 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:12 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:17 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:22 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:27 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 79.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:32 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:37 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:42 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:47 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:52 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:22:57 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:23:02 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:23:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 41.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:23:12 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:23:17 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:23:22 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:23:27 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:23:32 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: INFO 07-09 13:23:37 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "!docker run -it --rm --network=host \\\n",
    "  -v $(pwd)/models:/opt/ml/model/ \\\n",
    "  -v $(pwd)/logs:/opt/djl/logs \\\n",
    "  -v $(pwd)/neuron-cache:/var/tmp/neuron-compile-cache \\\n",
    "  -u djl \\\n",
    "  --device /dev/neuron0 \\\n",
    "  --device /dev/neuron1 \\\n",
    "  --device /dev/neuron2 \\\n",
    "  --device /dev/neuron3 \\\n",
    "  --device /dev/neuron4 \\\n",
    "  --device /dev/neuron5 \\\n",
    "  -e MODEL_LOADING_TIMEOUT=7200 \\\n",
    "  -e PREDICT_TIMEOUT=360 \\\n",
    "  {image_uri} serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61a226-3178-45a6-a2fb-9f7bec510370",
   "metadata": {},
   "source": [
    "## 추론 테스트\n",
    "Docker 엔드포인트가 생성된 후, Predictor 객체를 사용하여 Docker 엔드포인트에 대해 실시간 예측을 수행할 수 있습니다.\n",
    "- 추론 요청을 제출하고 응답을 받기 위해 `curl` 명령어를 사용합니다.\n",
    "- 요청과 응답은 JSON 형식으로 이루어집니다.\n",
    "\n",
    "아래 추론 코드를 별도의 터미널을 열고 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cac86-9b46-4d78-a914-2d0c82f58ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curl -N -X POST \"http://127.0.0.1:8080/predictions/llama3\" \\\n",
    "     -H 'Content-Type: application/json' \\\n",
    "     -d '{\n",
    "         \"seq_length\": 512,\n",
    "         \"inputs\": \"Welcome to Amazon Elastic Compute Cloud\",\n",
    "         \"parameters\": {\n",
    "             \"max_new_tokens\": 32,\n",
    "             \"do_sample\": \"true\"\n",
    "         }\n",
    "     }'"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
