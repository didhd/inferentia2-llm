{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74156eb6-9517-475f-984e-2e76d24fb281",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 실시간 엔드포인트를 DJL Serving을 이용해 호스팅\n",
    "이번 노트북에서는 [Deep Java Library](https://djl.ai/) (DJServing)가 지원하는 대형 모델 추론 컨테이너(LMI)를 모델 서빙 솔루션으로 사용합니다. \n",
    "\n",
    "DJL Serving은 별도의 사전 컴파일 없이도 HuggingFace SafeTensor 형식의 모델 웨이트를 자동으로 컴파일하고, 로드할 수 있도록 해 줍니다.\n",
    "\n",
    "AWS Neuron SDK는 사용자가 Inferentia 장치의 강력한 처리 능력을 쉽게 활용할 수 있게 해주며, DJLServing은 Java 기반 환경에서 대규모 모델을 손쉽게 서빙할 수 있도록 지원합니다. 이 노트북은 Amazon Elastic Compute Cloud(Amazon EC2) inf2.xlarge 인스턴스에 Llama 3 모델을 배포합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dff5e52d-2b6d-4e6e-b645-18c4174a8ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.ap-northeast-1.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원하는 리전으로 변경합니다.\n",
    "aws_region = \"ap-northeast-1\"\n",
    "\n",
    "# 컨테이너 이미지 확인\n",
    "image_uri = f\"763104351884.dkr.ecr.{aws_region}.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2\"\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b860ccb2-7580-46ba-a182-c090eac40cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e074430403247e69510de97708cf3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login, snapshot_download\n",
    "\n",
    "# Hugging Face 로그인\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8862d11c-34ac-4da0-8b4a-04d3bbced56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf19c0fd3bc24b8895fc76c0a50044ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/inferentia2-llm/models/llama3'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 다운로드\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "local_model_path = \"./models/llama3\"\n",
    "snapshot_download(repo_id=model_id, local_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a842f02a-3e2e-4d57-ab8e-472d9d89ea90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 서버에 필요한 serving.properties 파일 생성 (로컬 경로 참조)\n",
    "\n",
    "file_content = f\"\"\"engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=/opt/ml/model/llama3\n",
    "option.n_positions=8192\n",
    "option.rolling_batch=vllm\n",
    "option.max_rolling_batch_size=16\n",
    "option.tensor_parallel_degree=8\n",
    "option.enable_mixed_precision_accumulation=true\n",
    "option.load_split_model=false\n",
    "option.group_query_attention=replicated-heads\"\"\"\n",
    "\n",
    "with open(\"serving.properties\",\"w\") as f:\n",
    "    f.write(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f3adc854-4075-48fd-96b5-718aa20b6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 서버에 필요한 serving.properties 파일 생성 (로컬 경로 참조)\n",
    "\n",
    "# file_content = f\"\"\"engine=Python\n",
    "# option.entryPoint=djl_python.transformers_neuronx\n",
    "# option.model_id=/opt/ml/model/llama3\n",
    "# option.batch_size=1\n",
    "# option.neuron_optimize_level=2\n",
    "# option.load_in_8bit=false\n",
    "# option.n_positions=8192\n",
    "# option.rolling_batch=auto\n",
    "# option.tensor_parallel_degree=12\n",
    "# option.dtype=bf16\n",
    "# option.fuse_qkv=true\n",
    "# option.attention_layout=BSH\n",
    "# option.group_query_attention=replicated-heads\n",
    "# option.load_split_model=false\"\"\"\n",
    "\n",
    "# with open(\"serving.properties\",\"w\") as f:\n",
    "#     f.write(file_content)\n",
    "\n",
    "# scheduler: If you set engine to python and set rolling_batch to be auto or scheduler, we will use this. This is our default batch implementation focusing on HuggingFace Accelerate use cases.\n",
    "# lmi-dist: If you set engine to MPI and set rolling_batch to be auto or lmi-dist, we will use this. This is our default batch implementation focusing on tensor parallel use cases. It has flashAttention and PagedAttention implmentation, inspired from Text-Generation-Inference design\n",
    "# vllm: If you set engine to Python and set rolling_batch to be vllm, we will use this. This is our batch implementation to run vllm from its library, you also need to have a requirements.txt installing vllm==0.1.7 and pandas to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "502450fe-e7a5-4148-9554-d3a39480050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# `serving.properties`를 models/llama3 디렉토리로 이동\n",
    "mkdir -p logs\n",
    "mkdir -p neuron-cache\n",
    "cp serving.properties models/llama3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ece310d1-3922-433d-a21d-9de242e29c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.ap-northeast-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89a77b-8ca1-485d-8611-5d911c84b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO \u001b[m \u001b[92mEc2Utils\u001b[m DJL will collect telemetry to help us better understand our users? needs, diagnose issues, and deliver additional features. If you would like to learn more or opt-out please go to: https://docs.djl.ai/docs/telemetry.html for more information.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Starting model server ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Starting djl-serving: 0.28.0 ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m \n",
      "Model server home: /opt/djl\n",
      "Current directory: /opt/djl\n",
      "Temp directory: /tmp\n",
      "Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.28.0/conf/log4j2.xml -Xmx1g -Xms1g -Xss2m -XX:+ExitOnOutOfMemoryError\n",
      "Number of CPUs: 96\n",
      "Number of Neuron cores: 12\n",
      "Max heap size: 1024\n",
      "Config file: /opt/djl/conf/config.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Default job_queue_size: 1000\n",
      "Default batch_size: 1\n",
      "Default max_batch_delay: 100\n",
      "Default max_idle_time: 60\n",
      "Model Store: /opt/ml/model\n",
      "Initial Models: ALL\n",
      "Netty threads: 0\n",
      "Maximum Request Size: 67108864\n",
      "Environment variables:\n",
      "    HF_HOME: /tmp/.cache/huggingface\n",
      "    SERVING_FEATURES: vllm,lmi-dist,tnx\n",
      "    DJL_CACHE_DIR: /tmp/.djl.ai\n",
      "    OMP_NUM_THREADS: 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning for plugins...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning in plug-in folder :/opt/djl/plugins\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning in plug-in folder :/usr/local/djl-serving-0.28.0/plugins\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin console changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin static-file-plugin changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin cache-engines changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin secure-mode changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin kserve changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin console changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin static-file-plugin changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin cache-engines changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin secure-mode changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin kserve changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m 5 plug-ins found and loaded.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Found model llama3=file:/opt/ml/model/llama3/\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Initializing model: llama3=file:/opt/ml/model/llama3/\n",
      "\u001b[32mINFO \u001b[m \u001b[92mLmiUtils\u001b[m Detected mpi_mode: null, rolling_batch: vllm, tensor_parallel_degree 8, for modelType: llama\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m M-0001: Apply per model settings:\n",
      "    job_queue_size: 1000\n",
      "    max_dynamic_batch_size: 1\n",
      "    max_batch_delay: 100\n",
      "    max_idle_time: 60\n",
      "    load_on_devices: *\n",
      "    engine: Python\n",
      "    mpi_mode: null\n",
      "    option.entryPoint: djl_python.transformers_neuronx\n",
      "    option.load_split_model: false\n",
      "    option.n_positions: 8192\n",
      "    option.tensor_parallel_degree: 8\n",
      "    option.max_rolling_batch_size: 16\n",
      "    option.enable_mixed_precision_accumulation: true\n",
      "    option.group_query_attention: replicated-heads\n",
      "    option.model_id: /opt/ml/model/llama3\n",
      "    option.rolling_batch: vllm\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPlatform\u001b[m Found matching platform from: jar:file:/usr/local/djl-serving-0.28.0/lib/python-0.28.0.jar!/native/lib/python.properties\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python_engine.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/arg_parser.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/aws/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/aws/cloud_watch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/chat_completions/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/chat_completions/chat_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/chat_completions/chat_utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/encode_decode.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/huggingface.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/inputs.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/neuron_utils/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/neuron_utils/model_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/neuron_utils/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/np_util.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/output_formatter.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/outputs.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/pair_list.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/README.md to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/hf_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/lmi_dist_rb_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/scheduler_rb_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/sd_inf2_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/tnx_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/trt_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/properties_manager/vllm_rb_properties.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/request.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/request_io.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/lmi_dist_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/neuron_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/rolling_batch_vllm_utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/scheduler_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/trtllm_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/vllm_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/sagemaker.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/lm_block.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/search_config.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batch_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batcher.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batcher_impl.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/step_generation.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/service_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/session_manager.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/sm_log_filter.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/stable_diffusion_inf2.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/streaming_utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tensorrt_llm.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tensorrt_llm_python.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/test_model.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/optimum_modeling.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/slot.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/speculation.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/token_selector.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/ts_service_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelManager\u001b[m Loading model on Python:[nc0]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model llama3 (M-0001, PENDING) on nc(0) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m M-0001: Available CPU memory: 373703 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model llama3 M-0001 on nc(0)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19000 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=0-7\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set OMP_NUM_THREADS=16\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: 108 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--model-dir', '/opt/ml/model/llama3', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '0']\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stderr: /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stderr:   warnings.warn(\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: PJRT_DEVICE not set, defaulting to NEURON\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: WARNING 07-09 14:00:44 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For distributed inference, please install Ray with `pip install ray`.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: Model loading properties: model_id_or_path='/opt/ml/model/llama3' rolling_batch=<RollingBatchEnum.vllm: 'vllm'> tensor_parallel_degree=8 trust_remote_code=False enable_streaming=<StreamingEnum.false: 'false'> batch_size=16 max_rolling_batch_size=16 dtype=<Dtype.f16: 'fp16'> revision=None output_formatter=None waiting_steps=None mpi_mode=False tgi_compat=False draft_model_id=None spec_length=0 neuron_optimize_level=None enable_mixed_precision_accumulation=None enable_saturate_infinity=None n_positions=8192 unroll=None load_in_8bit=None low_cpu_mem_usage=False load_split_model=False context_length_estimate=None amp='f16' quantize=None compiled_graph_path=None draft_model_compiled_path=None speculative_draft_model=None speculative_length=5 draft_model_tp_size=None task=None save_mp_checkpoint_path=None group_query_attention='replicated-heads' model_loader=None rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'> fuse_qkv=None attention_layout=None collectives_layout=None cache_layout=None partition_schema=None all_reduce_dtype=None cast_logits_dtype=None on_device_embedding_config={}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: Start loading the model /opt/ml/model/llama3 using NeuronAutoModel...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: LLM sharding and compiling started...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: ..performing partition vectorization on AG_2[[0, 18, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_18_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 21, 0, 0, 0]]{3 nodes (1 sources, 0 stops)}. dags covered: {dag_27_TC_SRC, dag_21, dag_26_TRANSPOSE_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 12, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_12_TC_DST, dag_13_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 17, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_18_TRANSPOSE_SRC, dag_17_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 18, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_18_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 21, 0, 0, 0]]{3 nodes (1 sources, 0 stops)}. dags covered: {dag_27_TC_SRC, dag_21, dag_26_TRANSPOSE_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: ....performing partition vectorization on AG_2[[0, 11, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_11_TC_DST, dag_12_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 17, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_17_TRANSPOSE_DST, dag_18_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 20, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_20_TC_DST, dag_21_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 64, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_64_TC_DST, dag_65_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 69, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_70_TC_SRC, dag_69_TRANSPOSE_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 72, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_72_TC_DST, dag_73_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 115, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_115_TC_DST, dag_116_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 120, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_120_TRANSPOSE_DST, dag_121_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 123, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_123_TC_DST, dag_124_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 166, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_166_TC_DST, dag_167_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 171, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_172_TC_SRC, dag_171_TRANSPOSE_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 174, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_174_TC_DST, dag_175_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 217, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_217_TC_DST, dag_218_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 222, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_222_TRANSPOSE_DST, dag_223_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 225, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_225_TC_DST, dag_226_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 268, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_268_TC_DST, dag_269_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 273, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_273_TRANSPOSE_DST, dag_274_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 276, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_277_TRANSPOSE_SRC, dag_276_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 319, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_319_TC_DST, dag_320_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 324, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_324_TRANSPOSE_DST, dag_325_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 327, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_328_TRANSPOSE_SRC, dag_327_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 370, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_370_TC_DST, dag_371_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 375, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_375_TRANSPOSE_DST, dag_376_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 378, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_379_TRANSPOSE_SRC, dag_378_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 421, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_421_TC_DST, dag_422_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 426, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_426_TRANSPOSE_DST, dag_427_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 429, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_430_TRANSPOSE_SRC, dag_429_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 472, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_472_TC_DST, dag_473_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 477, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_478_TC_SRC, dag_477_TRANSPOSE_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 480, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_480_TC_DST, dag_481_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 523, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_523_TC_DST, dag_524_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 528, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_528_TRANSPOSE_DST, dag_529_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 531, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_531_TC_DST, dag_532_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 574, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_574_TC_DST, dag_575_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 579, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_580_TC_SRC, dag_579_TRANSPOSE_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 582, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_582_TC_DST, dag_583_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 625, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_625_TC_DST, dag_626_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 630, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_630_TRANSPOSE_DST, dag_631_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 633, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_633_TC_DST, dag_634_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 676, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_676_TC_DST, dag_677_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 681, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_681_TRANSPOSE_DST, dag_682_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 684, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_685_TRANSPOSE_SRC, dag_684_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 727, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_727_TC_DST, dag_728_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 732, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_732_TRANSPOSE_DST, dag_733_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 735, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_736_TRANSPOSE_SRC, dag_735_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 778, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_778_TC_DST, dag_779_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 783, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_783_TRANSPOSE_DST, dag_784_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 786, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_787_TRANSPOSE_SRC, dag_786_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 829, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_830_TRANSPOSE_SRC, dag_829_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 834, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_834_TRANSPOSE_DST, dag_835_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 837, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_837_TC_DST, dag_838_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 880, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_881_TRANSPOSE_SRC, dag_880_TC_DST}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 885, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_885_TRANSPOSE_DST, dag_886_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 888, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_888_TC_DST, dag_889_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 931, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_931_TC_DST, dag_932_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 936, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_936_TRANSPOSE_DST, dag_937_TC_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: performing partition vectorization on AG_2[[0, 939, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_939_TC_DST, dag_940_TRANSPOSE_SRC}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: \n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-108-llama3-stdout: Compiler status PASS\n"
     ]
    }
   ],
   "source": [
    "!docker run -it --rm --network=host \\\n",
    "  -v $(pwd)/models:/opt/ml/model/ \\\n",
    "  -v $(pwd)/logs:/opt/djl/logs \\\n",
    "  -v $(pwd)/neuron-cache:/var/tmp/neuron-compile-cache \\\n",
    "  -u djl \\\n",
    "  --device /dev/neuron0 \\\n",
    "  --device /dev/neuron1 \\\n",
    "  --device /dev/neuron2 \\\n",
    "  --device /dev/neuron3 \\\n",
    "  --device /dev/neuron4 \\\n",
    "  --device /dev/neuron5 \\\n",
    "  -e MODEL_LOADING_TIMEOUT=7200 \\\n",
    "  -e PREDICT_TIMEOUT=360 \\\n",
    "  {image_uri} serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61a226-3178-45a6-a2fb-9f7bec510370",
   "metadata": {},
   "source": [
    "## 추론 테스트\n",
    "Docker 엔드포인트가 생성된 후, Predictor 객체를 사용하여 Docker 엔드포인트에 대해 실시간 예측을 수행할 수 있습니다.\n",
    "- 추론 요청을 제출하고 응답을 받기 위해 `curl` 명령어를 사용합니다.\n",
    "- 요청과 응답은 JSON 형식으로 이루어집니다.\n",
    "\n",
    "아래 추론 코드를 별도의 터미널을 열고 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cac86-9b46-4d78-a914-2d0c82f58ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curl -N -X POST \"http://127.0.0.1:8080/predictions/llama3\" \\\n",
    "     -H 'Content-Type: application/json' \\\n",
    "     -d '{\n",
    "         \"seq_length\": 512,\n",
    "         \"inputs\": \"Welcome to Amazon Elastic Compute Cloud\",\n",
    "         \"parameters\": {\n",
    "             \"max_new_tokens\": 32,\n",
    "             \"do_sample\": \"true\"\n",
    "         }\n",
    "     }'"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
