{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74156eb6-9517-475f-984e-2e76d24fb281",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 실시간 엔드포인트를 DJL Serving을 이용해 호스팅\n",
    "이번 노트북에서는 [Deep Java Library](https://djl.ai/) (DJL Serving)가 지원하는 대형 모델 추론 컨테이너(LMI)를 모델 서빙 솔루션으로 사용합니다. \n",
    "DJL Serving은 [transformers-neuronx](https://github.com/aws-neuron/transformers-neuronx) 라이브러리와 통합되어 있고, 모델 서버에 Neuron SDK가 미리 포함되어 있어 컴파일 작업을 사전 수행할 수 있습니다. 또한 뉴런 코어 간의 텐서 병렬 처리를 지원하고 있어 여러 뉴런 코어에 걸쳐 모델을 병렬로 로드할 수 있습니다. 이 노트북은 Amazon Elastic Compute Cloud(Amazon EC2) inf2.24xlarge 인스턴스에 Llama 3 모델을 배포합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5e52d-2b6d-4e6e-b645-18c4174a8ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 원하는 리전으로 변경합니다.\n",
    "aws_region = \"ap-northeast-1\"\n",
    "\n",
    "# 컨테이너 이미지 확인\n",
    "image_uri = f\"763104351884.dkr.ecr.{aws_region}.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2\"\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27a3d2-5ba6-4607-ae22-b7f3ce51e1ff",
   "metadata": {},
   "source": [
    "### Llama3 모델 다운로드\n",
    "Llama3 모델을 사용하기 위한 첫 단계는 모델을 다운로드하는 것입니다. 이 과정은 Hugging Face Hub에서 제공하는 편리한 도구들을 활용하여 수행할 수 있습니다. 먼저, Hugging Face Hub에 로그인해야 합니다. 이는 huggingface_hub 라이브러리의 login() 함수를 통해 간단히 수행할 수 있습니다. 셸 실행 후 read 권한이 있는 token을 입력합니다. huggingface에서 token을 발급 받는 방법은 [여기](https://huggingface.co/docs/hub/security-tokens)를 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860ccb2-7580-46ba-a182-c090eac40cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face 로그인\n",
    "from huggingface_hub import login, snapshot_download\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a75628-3a2e-49d2-ab77-ea3322cc9b20",
   "metadata": {},
   "source": [
    "이 명령을 실행하면 로그인 창이 열리고 Hugging Face 계정으로 로그인할 수 있습니다. 로그인 과정은 모델에 접근하기 위한 인증을 제공합니다.\n",
    "토큰 인증이 완료되면 이제 Llama3 모델을 다운로드할 차례입니다. 이를 위해 `snapshot_download()` 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862d11c-34ac-4da0-8b4a-04d3bbced56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 다운로드\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "local_model_path = \"./models/llama3\"\n",
    "snapshot_download(repo_id=model_id, local_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff234908-8d7a-41e3-bbc2-828796cd4a08",
   "metadata": {},
   "source": [
    "### 모델 서빙 아티팩트 준비하기\n",
    "LMI 컨테이너는 Amazon S3 또는 Hugging Face Hub에 등록된 모델, 그리고 로컬 모델을 로드하도록 설정할 수 있습니다. 모델을 로드하고 호스팅하기 위해 serving.properties 파일에 필요한 파라미터들이 필요합니다. 구성 가능한 파라미터의 전체 목록은 [Transformers-NeuronX Engine in LMI 페이지의 serving.properties 항목](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/tnx_user_guide.html#servingproperties)을 참조하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842f02a-3e2e-4d57-ab8e-472d9d89ea90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 서버에 필요한 serving.properties 파일 생성 (로컬 경로 참조)\n",
    "\n",
    "file_content = f\"\"\"engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=/opt/ml/model/llama3\n",
    "option.n_positions=8192\n",
    "option.rolling_batch=vllm\n",
    "option.neuron_optimize_level=1\n",
    "option.max_rolling_batch_size=16\n",
    "option.tensor_parallel_degree=8\n",
    "option.dtype=fp16\n",
    "option.enable_mixed_precision_accumulation=true\n",
    "option.load_split_model=false\n",
    "option.fuse_qkv=true\n",
    "option.attention_layout=BSH\n",
    "option.load_in_8bit=true\n",
    "option.group_query_attention=replicated-heads\n",
    "\"\"\"\n",
    "\n",
    "with open(\"serving.properties\",\"w\") as f:\n",
    "    f.write(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e30cfe-4f1b-43c6-95fa-d0fde06a7c4f",
   "metadata": {},
   "source": [
    "`serving.properties`를 models/llama3 디렉토리로 이동시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502450fe-e7a5-4148-9554-d3a39480050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# `serving.properties`를 models/llama3 디렉토리로 이동\n",
    "mkdir -p logs\n",
    "mkdir -p neuron-cache\n",
    "cp serving.properties models/llama3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e32db-f4b6-42af-9aa3-b450bd6e3cb3",
   "metadata": {},
   "source": [
    "다음으로, 앞서 정의한 모델 설정을 사용하여 Docker를 이용한 엔드포인트를 생성합니다. 모델 배포는 보통 4-5분 정도 소요되며, 이 과정에서 모델이 컴파일됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece310d1-3922-433d-a21d-9de242e29c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.ap-northeast-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da74363-ab84-4560-8429-dd7f190d7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm --network=host \\\n",
    "  -v $(pwd)/models:/opt/ml/model/ \\\n",
    "  -v $(pwd)/logs:/opt/djl/logs \\\n",
    "  -v $(pwd)/neuron-cache:/var/tmp/neuron-compile-cache \\\n",
    "  -u djl \\\n",
    "  --device /dev/neuron0 \\\n",
    "  --device /dev/neuron1 \\\n",
    "  --device /dev/neuron2 \\\n",
    "  --device /dev/neuron3 \\\n",
    "  --device /dev/neuron4 \\\n",
    "  --device /dev/neuron5 \\\n",
    "  -e MODEL_LOADING_TIMEOUT=7200 \\\n",
    "  -e PREDICT_TIMEOUT=360 \\\n",
    "  {image_uri} serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26749aa-aff2-4ee9-a792-209eeb57e506",
   "metadata": {},
   "source": [
    "### 추론 테스트\n",
    "Docker 엔드포인트가 생성된 후, curl 명령어를 사용하여 Docker 엔드포인트에 대해 실시간 예측을 수행할 수 있습니다.\n",
    "\n",
    "* 추론 요청을 제출하고 응답을 받기 위해 curl 명령어를 사용합니다.\n",
    "* 요청과 응답은 JSON 형식으로 이루어집니다.\n",
    "\n",
    "별도의 터미널을 열고 모델 서버에 아래 명령어를 입력하고 추론 결과를 받아봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cac86-9b46-4d78-a914-2d0c82f58ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curl -N -X POST \"http://127.0.0.1:8080/predictions/llama3\" \\\n",
    "     -H 'Content-Type: application/json' \\\n",
    "     -d '{\n",
    "         \"seq_length\": 512,\n",
    "         \"inputs\": \"Welcome to Amazon Elastic Compute Cloud\",\n",
    "         \"parameters\": {\n",
    "             \"max_new_tokens\": 32,\n",
    "             \"do_sample\": \"true\"\n",
    "         }\n",
    "     }'"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
