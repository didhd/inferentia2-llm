{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef5644c-2f34-4cca-861a-f11548a67caa",
   "metadata": {},
   "source": [
    "## 성능 측정\n",
    "LLama3 모델을 Inferentia2에 성공적으로 배포하였으니, 이제 이 모델의 성능을 벤치마킹하기 위해 코드를 다운로드하고 모델의 성능을 측정할 준비를 합니다. 다음 명령어를 통해 벤치마크를 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1c960-a40c-4548-9aa0-0c9474fd5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark.py \\\n",
    "  --endpoint http://127.0.0.1:8080/predictions/llama3 \\\n",
    "  --num-concurrent-requests 20 \\\n",
    "  --max-num-completed-requests 100 \\\n",
    "  --input-tokens 4096 \\\n",
    "  --max-new-tokens 4096 \\\n",
    "  --timeout 600 \\\n",
    "  --results-dir results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62efe7df-79aa-400a-82fa-e7feea7e3e4e",
   "metadata": {},
   "source": [
    "이 명령은 동시에 10명의 사용자가 100개의 요청을 처리하면서, 모델의 첫 토큰까지의 시간, 토큰당 지연 시간(ms/token) 및 처리량(tokens/s)을 측정합니다. 이 모든 상세 결과는 결과 폴더에서 확인할 수 있습니다.\n",
    "\n",
    "이제 결과를 파싱하고 표시해 보겠습니다. 다음은 Python 코드와 그 출력 결과입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c3cc1-a176-41d5-bfce-b86f1009e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 폴더에서 summary.json 파일을 읽고 결과를 출력\n",
    "import glob\n",
    "import json\n",
    "\n",
    "with open(glob.glob('results/benchmark_summary*.json')[0], 'r') as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(f\"Concurrent requests: {results['num_concurrent_requests']}\")\n",
    "print(f\"Input tokens: {results['input_tokens']}\")\n",
    "print(f\"Max new tokens: {results['max_new_tokens']}\")\n",
    "print(f\"Avg. Latency: {results['results']['avg_latency']:.2f}ms\")\n",
    "print(f\"Avg. Time-to-First-Token (TTFT): {results['results']['avg_ttft']:.2f}ms\")\n",
    "print(f\"Output Token Throughput: {results['results']['output_tokens_per_second']:.2f} tokens/sec\")\n",
    "print(f\"Requests per second: {results['results']['requests_per_second']:.2f}\")\n",
    "print(f\"Total time: {results['results']['total_time']:.2f}s\")\n",
    "print(f\"Total output tokens generated: {results['results']['total_output_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4299c3-954f-4306-9fc8-188bbc5b06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrent requests: 25\n",
    "# Avg. Input token length: 512.0\n",
    "# Avg. Output token length: 27.0\n",
    "# Avg. First-Time-To-Token: 469.89ms\n",
    "# Avg. Throughput: 57.45 tokens/sec\n",
    "# Avg. Latency: 28.02ms/token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
