{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0bbcac-b9ce-4d74-a8e6-6fc2364c1ce0",
   "metadata": {},
   "source": [
    "## Transformer NeuronX를 이용해 모델 컴파일 \n",
    "\n",
    "가장 먼저 이용할 수 있는 옵션은, Transformer NeuronX 라이브러리의 [컴파일](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/neuronx-cc/api-reference-guide/neuron-compiler-cli-reference-guide.html#neuron-compiler-cli-reference-guide) 기능을 이용해 모델을 컴파일 하는 것입니다. 성능을 개선하기 위해 몇 가지 추가 구성을 설정할 수 있습니다.\n",
    "\n",
    "* attention_layout: Attention 계산에 사용할 레이아웃으로 \"BSH\"를 사용합니다.\n",
    "* fuse_qkv: QKV 투영을 단일 행렬 곱셈으로 융합하여 Q/K/V 가중치 로딩 효율성을 높입니다.\n",
    "* group_query_attention: KV 캐시 샤딩 전략에 대한 자세한 내용은 transformers neuronx의 Grouped Query Attention을 참조하세요. (See Appendix. 4)\n",
    "* quant: 모델의int8 가중치 저장은 MLP 레이어의 로드 시간을 개선할 수 있습니다. \n",
    "* on_device_generation: Transformers Neuron은 장치에서 top-k 샘플링을 지원하며, 이는 성능을 향상시키는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619c861-0691-4410-a901-d7292368ebb1",
   "metadata": {},
   "source": [
    "Llama 3 모델의 사용은 Meta 라이선스에 의해 규제되며, 이 샘플을 실행하기 전에 다운로드해야 합니다. Meta로부터 Llama 3 모델에 접근하기 위한 단계는 [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)에 설명되어 있습니다.\n",
    "\n",
    "이 노트북을 실행하기 위해 [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) 모델을 Hugging Face에서 접근할 수 있어야 합니다. 이후 HuggingFace에 로그인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c3c4b6-2d3d-49d5-8b9d-afb4b7114657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91064553aa94a5281b27ab8e00971d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2744bb9-2047-4a26-aaf4-b6f2fbc0aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "모델을 컴파일 하기 전, 해당 모델의 적정 batch_size와 tp_degree, n_positions를 확인한 후 아래 코드를 업데이트 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c00497-fe77-4967-a4b3-56d33f0fdf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx import LlamaForSampling\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast\n",
    "from transformers_neuronx import LlamaForSampling, NeuronConfig, GQA, QuantizationConfig\n",
    "from transformers_neuronx.config import GenerationConfig \n",
    "\n",
    "# Hugging Face 모델 ID 설정\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B/\"\n",
    "\n",
    "neuron_config = NeuronConfig(\n",
    "                    on_device_embedding=False,\n",
    "                    attention_layout='BSH',\n",
    "                    fuse_qkv=True,\n",
    "                    group_query_attention=GQA.REPLICATED_HEADS,\n",
    "                    quant=QuantizationConfig(quant_dtype='s8', dequant_dtype='f16'),\n",
    "                    on_device_generation=GenerationConfig(do_sample=True)\n",
    "              )\n",
    "\n",
    "# meta-llama/Meta-Llama-3-8B 모델을 NeuronCores에 12-way 텐서 병렬 처리로 로드하고 컴파일 실행\n",
    "neuron_model = LlamaForSampling.from_pretrained(model_id, neuron_config=neuron_config, batch_size=1, tp_degree=12, amp='f16', n_positions=8192)\n",
    "neuron_model.to_neuron() # 모델 로드/컴파일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6883c9c-5671-4f46-b178-a12c87e257f2",
   "metadata": {},
   "source": [
    "Neuron LlamaForSampling 클래스를 사용하여 Llama 3 모델에 텐서 병렬 처리를 구현하는 방법에 대해 알아보겠습니다. 이 기술을 적용하려면, 모델의 시나리오에 맞는 시퀀스 길이로 `n_positions`를 설정해야 합니다. 이 값은 예상되는 입력 및 출력 토큰의 합보다 반드시 커야 합니다. 텐서 병렬 처리는 `tp_degree=12` 인수를 통해 활성화됩니다.\n",
    "\n",
    "성능을 높이기 위해 `amp='f16'` 플래그를 사용하여 모델을 float16으로 캐스팅할 수 있습니다. 이렇게 하면 모델의 계산 효율성이 향상되어, 리소스 사용을 최적화하며 더 빠른 추론 속도를 얻을 수 있습니다.\n",
    "\n",
    "마지막으로, 모델의 계산 그래프는 Neuron에서 최적화된 추론을 위해 neuronx-cc에 의해 컴파일됩니다. 이 과정을 통해 모델은 AWS Neuron 하드웨어에서 최적의 성능을 발휘할 수 있도록 준비됩니다. 이러한 접근 방식은 딥러닝 모델의 효율과 성능을 극대화하는 데 큰 도움이 됩니다.\n",
    "\n",
    "모델을 한 번 컴파일하고 나면, 해당 결과물을 save 함수를 사용하여 지정된 디렉토리에 저장할 수 있습니다. 이렇게 저장된 모델 아티팩트는 나중에 load 함수를 통해 쉽게 불러올 수 있으며, 이 과정은 모델을 다시 컴파일할 필요 없이 빠르게 모델을 배포할 수 있게 해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6418e6-4c64-4729-a7a7-4651e308cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_model.save('./neuron_artifacts') # can be copied and used on a different neuron instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c78aedc-48f7-45ce-b7eb-110f5f1e25fc",
   "metadata": {},
   "source": [
    "컴파일이 완료되면, 다음과 같은 형태의 디렉토리 구조가 완성이 됩니다.\n",
    "\n",
    "```plaintext\n",
    "neuron_artifacts/\n",
    "├── 14c0de162adad18e6e77.neff\n",
    "├── 1cabdc562e64e63d0081.neff\n",
    "├── 641d2daf2e65a013406d.neff\n",
    "├── 7ad05afb0577f47d52db.neff\n",
    "├── 8159e002a19b6ddfe12f.neff\n",
    "├── 89e76df03b41e6cc5d22.neff\n",
    "├── 9340a6b565a68744724c.neff\n",
    "├── c637f30abbd2f600a421.neff\n",
    "├── c820947075ac802b30f3.neff\n",
    "├── ccbc3860b984ace7420a.neff\n",
    "├── d96191b583dde6135aa9.neff\n",
    "└── de011594ed2f0cc29300.neff\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
